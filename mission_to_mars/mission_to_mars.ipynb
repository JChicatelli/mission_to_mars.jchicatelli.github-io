{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from splinter import Browser\n",
    "import pandas as pd\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {\"executable_path\": \"/usr/local/bin/chromedriver\"}\n",
    "browser = Browser(\"chrome\", **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_news_url = \"https://mars.nasa.gov/news/\"\n",
    "browser.visit(mars_news_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "mars_news = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use prettify to analysze the html \n",
    "print(mars_news.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the above results, then determine element that contains sought info\n",
    "# Get the first title by returning div element the class = content_title \n",
    "\n",
    "news_title = mars_news.find('div', class_='content_title').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After examing the results above, get the first paragraph text by returning the div element \n",
    "# with the class = rollover_description_inner.\n",
    "\n",
    "news_p = mars_news.find('div', class_=\"rollover_description_inner\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The latest Mars news is:\",news_title)\n",
    "print(\"The summary of this latest news is:\",news_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While chromedriver is open go to JPL's Featured Space Image page. \n",
    "jpl_url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "browser.visit(jpl_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the browser into soup and use soup to find the full resolution image of mars\n",
    "# Save the image url to a variable called `featured_image_url`\n",
    "html = browser.html\n",
    "jpl_soup = bs(html, 'html.parser')\n",
    "image_url = jpl_soup.find('a', {'id': 'full_image', 'data-fancybox-href': True}).get('data-fancybox-href')\n",
    "image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the base url from the href of the website \n",
    "jpl_logo_href = jpl_soup.find_all('div', class_='jpl_logo')\n",
    "print(jpl_logo_href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "html_page = browser.html\n",
    "JPL_soup = bs(html_page, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the hrefs of the url\n",
    "links = []\n",
    "for link in JPL_soup.find_all('a'):\n",
    "    links.append(link.get('href'))\n",
    " \n",
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the 2nd href in the list \n",
    "jpl_link = links[1].strip('/')\n",
    "print(jpl_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_image_url = \"https://\"+jpl_link+image_url\n",
    "print(featured_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While chromedriver is open go to Mars weathe twitter page. \n",
    "twitter_url = \"https://twitter.com/marswxreport?lang=en\"\n",
    "browser.visit(twitter_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "twitter_news = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = twitter_news.find('p', class_='TweetTextSize TweetTextSize--normal js-tweet-text tweet-text')\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_weather = weather.text.strip()\n",
    "mars_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  While chromedriver is open go to Mars facts url using chrome from the excuteable command above. \n",
    "mars_facts_url = \"https://space-facts.com/mars/\"\n",
    "browser.visit(mars_facts_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "mars_facts = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the url to a pandas df\n",
    "mars_df = pd.read_html(mars_facts_url)\n",
    "mars_facts_df = pd.DataFrame(mars_df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns and set the index.  \n",
    "mars_facts_df.columns = ['Characteristic','Data']\n",
    "mars_df_table = mars_facts_df.set_index(\"Characteristic\")\n",
    "mars_df_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pd df to HTML table and clean up. \n",
    "mars_html_table = mars_df_table.to_html(classes='marsdata')\n",
    "mars_table = mars_html_table.replace('\\n', ' ')\n",
    "mars_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  While chromedriver is open go to USGS Astr ogeologyurl using chrome from the excuteable command above. \n",
    "hemispheres_url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "browser.visit(hemispheres_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "mars_hemispheres = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mars_hemispheres.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the div element that holds the images. \n",
    "images = mars_hemispheres.find('div', class_='collapsible results')\n",
    "print(images.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through the class=\"item\" by clicking the h3 tag and getting the title and url. \n",
    "\n",
    "hemispheres_image_urls = []\n",
    "\n",
    "for i in range(len(images.find_all(\"div\", class_=\"item\"))):\n",
    "    time.sleep(5)\n",
    "    image = browser.find_by_tag('h3')\n",
    "    image[i].click()\n",
    "    html = browser.html\n",
    "    soup = bs(html, 'html.parser')\n",
    "    title = soup.find(\"h2\", class_=\"title\").text\n",
    "    div = soup.find(\"div\", class_=\"downloads\")\n",
    "    for li in div:\n",
    "               link = div.find('a')\n",
    "    url = link.attrs['href']\n",
    "    hemispheres = {\n",
    "            'title' : title,\n",
    "            'img_url' : url\n",
    "        }\n",
    "    hemispheres_image_urls.append(hemispheres)\n",
    "    browser.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemispheres_image_urls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
